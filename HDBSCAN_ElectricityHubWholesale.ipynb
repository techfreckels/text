{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "HDBSCAN_ElectricityHubWholesale.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/techfreckels/text/blob/master/HDBSCAN_ElectricityHubWholesale.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "75b885b3-ea33-4784-9357-b28165c85c41",
        "_uuid": "e19f38ba3db998770dbc30c0b996b7e28ccaf87d",
        "id": "K_S-mZGkw2sV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "from sklearn.datasets.samples_generator import make_blobs\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import itertools\n",
        "import datetime as dt\n",
        "import statsmodels as sm\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('fivethirtyeight')\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set_context('poster')\n",
        "sns.set_style('white')\n",
        "sns.set_color_codes()\n",
        "plot_kwds = {'alpha' : 0.5, 's' : 80, 'linewidths':0}\n",
        "!pip install hdbscan\n",
        "import hdbscan\n",
        "plt.rcParams[\"figure.figsize\"] = (16,12)\n",
        "\n",
        "from IPython.display import HTML\n",
        "css_str = '<style>ul {  list-style-type: none;  margin: 0;  padding: 0;}</style>'\n",
        "html = HTML(css_str)\n",
        "display(html)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMDyNU6jw2tS",
        "colab_type": "text"
      },
      "source": [
        "# Unsupervised anomaly detection in electricity markets by HDBSCAN clustering method\n",
        "HDBSCAN is a clustering algorithm developed by Campello, Moulavi, and Sander. It HDBSCAN stands for <i>Hierarchical Density-Based Spatial Clustering of Applications with Noise</i>.<br>\n",
        "\n",
        "## No (few) assumptions except for some noise\n",
        "We want to have as few assumptions about our data as possible. Perhaps the only assumptions that we can safely make are:\n",
        "<ul>\n",
        "<li>\n",
        "    There is noise in our data\n",
        "    </li>\n",
        "    <li>\n",
        "    There are clusters in our data which we hope to discover\n",
        "    </li>\n",
        "</ul> \n",
        "\n",
        "## Clustering dataset\n",
        "To motivate our discussion, we start with a toy data set.\n",
        "<img src=\"https://miro.medium.com/max/715/1*36x7yPCJGVrKnogBpE2n4w.png\"/>\n",
        "We can plot the data and identify 6 \"natural\" clusters in our dataset. We hope to automatically identify these through some clustering algorithm.\n",
        "## K-means vs HDBSCAN\n",
        "Knowing the expected number of clusters, we run the classical K-means algorithm and compare the resulting labels with those obtained using HDBSCAN.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/2038/1*L-hr07E_ygPJEqDXgaoGQA.png\"/>\n",
        "\n",
        "Even when provided with the correct number of clusters, K-means clearly fails to group the data into useful clusters. HDBSCAN, on the other hand, gives us the expected clustering.\n",
        "\n",
        "## Why does K-means fail?\n",
        "\n",
        "\n",
        "Let us borrow a simpler example from ESLR [4] to illustrate how K-means can be sensitive to the shape of the clusters. Below are two clusterings from the same data. On the left, data was standardized before clustering. Without standardization, we get a “wrong” clustering.\n",
        "\n",
        "\n",
        "\n",
        "Briefly, K-means performs poorly because the underlying assumptions on the shape of the clusters are not met; it is a parametric algorithm parameterized by the K cluster centroids, the centers of gaussian spheres. K-means performs best when clusters are:\n",
        "<ul>\n",
        "    <li>\"round\" or spherical</li>\n",
        "    <li>equally sized </li>\n",
        "    <li>equally dense </li>\n",
        "    <li>most dense in the center of the sphere </li>\n",
        "    <li>not contaminated by noise/outliers </li>\n",
        "</ul>\n",
        "\n",
        "Let us borrow a simpler example from ESLR to illustrate how K-means can be sensitive to the shape of the clusters. Below are two clusterings from the same data. On the left, data was standardized before clustering. Without standardization, we get a \"wrong\" clustering.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1662/1*gzRRGby6vq6buR1SlzGaJg.png\" />\n",
        "\n",
        "\n",
        "## What are the characteristics of our data?\n",
        "\n",
        "We go back to our original data set and by simply describing it, it becomes obvious why K-means has a hard time. The data set has:\n",
        "<ul>\n",
        "    <li>\n",
        "    Clusters with arbitrary shapes</li>\n",
        "    <li>Clusters of different sizes</li>\n",
        "    <li>Clusters with different densities</li>\n",
        "    <li>Some noise and maybe some outliers</li>\n",
        "</ul>\n",
        "<img src=\"https://miro.medium.com/max/1408/1*nHCw-IeJvNWSm4iq0UkZNg.png\" />\n",
        "\n",
        "## Need robustness for data exploration\n",
        "\n",
        "While each bullet point can be reasonably expected from a real-world dataset, each one can be problematic for parametric algorithms such as K-means. We might want to check if the assumptions of our algorithms are met before trusting their output. But, checking for these assumptions can be difficult when little is known about the data. This is unfortunate because one of the primary uses of clustering algorithms is data exploration where we are still in the process of understanding the data\n",
        "\n",
        "Therefore, a clustering algorithm that will be used for data exploration needs to have as few assumptions as possible so that the initial insights we get are “useful”; having fewer assumptions make it more robust and applicable to a wider range of real-world data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Y801_t9w2tr",
        "colab_type": "text"
      },
      "source": [
        "# Dense regions and multivariate modes\n",
        "Now, we have an idea what type of data we are dealing with, let’s explore the core ideas of HDBSCAN and how it excels even when the data has:\n",
        "<ul>\n",
        "    <li>\n",
        "    Arbitrarily shaped clusters</li>\n",
        "    <li>Clusters with different sizes and densities</li>\n",
        "    <li>Noise</li>\n",
        "\n",
        "HDBSCAN uses a density-based approach which makes few implicit assumptions about the clusters. It is a non-parametric method that looks for a cluster hierarchy shaped by the multivariate modes of the underlying distribution. Rather than looking for clusters with a particular shape, it looks for regions of the data that are denser than the surrounding space. The mental image you can use is trying to separate the islands from the sea or mountains from its valleys.\n",
        "    \n",
        "## What’s a cluster?\n",
        "\n",
        "How do we define a “cluster”? The characteristics of what we intuitively think as a cluster can be poorly defined and are often context-specific. (See Christian Hennig’s talk [5] for an overview)\n",
        "\n",
        "If we go back to the original data set, the reason we identify clusters is that we see 6 dense regions surrounded by sparse and noisy space.\n",
        "    \n",
        "    <img src=\"https://miro.medium.com/max/1408/1*eStGcmNGVN3-WC2IcEDY4A.png\" />\n",
        "    \n",
        "One way of defining a cluster which is usually consistent with our intuitive notion of clusters is: highly dense regions separated by sparse regions.\n",
        "\n",
        "Look at the plot of 1-d simulated data. We can see 3 clusters.\n",
        "  <img src=\"https://miro.medium.com/max/3243/1*xyD-oZmG6tGcAAXyxrn72g.png\" />\n",
        "    \n",
        "## Looking at the underlying distribution\n",
        "\n",
        "X is simulated data from a mixture of normal distributions, and we can plot the exact probability distribution of X.    \n",
        "  <img src=\"https://miro.medium.com/max/1490/1*naEKid6E2eO43jgLsIhGmA.png\" />\n",
        "The peaks correspond to the densest regions and the troughs correspond to the sparse regions. This gives us another way of framing the problem assuming we know the underlying distribution, clusters are highly probable regions separated by improbable regions. Imagine the higher-dimensional probability distributions forming a landscape of mountains and valleys, where the mountains are your clusters.\n",
        "    <img src=\"https://miro.medium.com/max/1490/1*W3kun_Pxbmgn6S_-ZzHZAA.png\" />\n",
        "    For those not as familiar, the two statements are practically the same:\n",
        "<ul>\n",
        "    <li>\n",
        "    highly dense regions separated by sparse regions </li>\n",
        "     <li>highly probable regions separated by improbable regions <li>\n",
        "</ul>\n",
        "One describes the data through its probability distribution and the other through a random sample from that distribution.\n",
        "\n",
        "The PDF plot and the strip plot above are equivalent. PDF, probability density function, is interpreted as the probability at that point, or a small region around it, and when looking at a sample from X, it can also be interpreted as the expected density around that point.\n",
        "\n",
        "Given the underlying distribution, we expect that regions that are more probable would tend to have more points (denser) in a random sample. Similarly, given a random sample, you can make inferences on the probability of a region based on the empirical density.\n",
        "    \n",
        "## Denser regions in the random sample correspond to more probable regions in the underlying distributions.\n",
        "\n",
        "In fact, if we look at the histogram of a random sample of X, we see that it looks exactly like the true distribution of X. The histogram is sometimes called the empirical probability distribution, and with enough data, we expect the histogram to converge to the true underlying distribution.    \n",
        "    \n",
        " <img src=\"https://miro.medium.com/max/1524/1*W75kS8rV_3sVmfOMojPDFA.png\" />\n",
        "Again, density = probability. Denser = more probable.\n",
        "\n",
        "    \n",
        "## But… what’s a cluster?\n",
        "\n",
        "Sadly, even with our “mountains and valleys” definition of clusters, it can be difficult to know whether or not something is a single cluster. Look at the example below where we shifted one of the modes of X to the right. Although we still have 3 peaks, do we have 3 clusters? In some contexts, we might consider 3 clusters. “Intuitively” we say there are just 2 clusters. How do we decide?    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J37TBMqkw2t0",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "data = pd.read_csv(\"/home/mcfly/Documents/mari/ts_ercot.csv\")\n",
        "import plotly.offline as pyo\n",
        "import plotly.graph_objs as go\n",
        "pyo.init_notebook_mode()\n",
        "\n",
        "#import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=data[\"Date\"],\n",
        "    y=data[\"Price\"],\n",
        "    name=\"Price\"       # this sets its legend entry\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"ERCOT\",\n",
        "    xaxis_title=\"Date\",\n",
        "    yaxis_title=\"Price\",\n",
        "    font=dict(\n",
        "        family=\"Courier New, monospace\",\n",
        "        size=14,\n",
        "        color=\"#7f7f7f\"\n",
        "    )\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3J-SUb9w2ui",
        "colab_type": "text"
      },
      "source": [
        "So we have a total of three clusters, with labels 0, 1, and 2. Importantly HDBSCAN is noise aware – it has a notion of data samples that are not assigned to any cluster. This is handled by assigning these samples the label -1. But wait, there’s more. The hdbscan library implements soft clustering, where each data point is assigned a cluster membership score ranging from 0.0 to 1.0. A score of 0.0 represents a sample that is not in the cluster at all (all noise points will get this score) while a score of 1.0 represents a sample that is at the heart of the cluster (note that this is not the spatial centroid notion of core). You can access these scores via the probabilities_ attribute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "7ad31c20-66f3-4059-b0ef-a977c4288d51",
        "_uuid": "da954ea66d76659fddd4722c4b9b3a74f124202c",
        "id": "aQSPmBQZw2ut",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = []\n",
        "x = []\n",
        "y=[]\n",
        "for i in range(0, len(data)-1):\n",
        "    a.append([i,data[\"Price\"][i]])    \n",
        "\n",
        "X = a\n",
        "X = StandardScaler().fit_transform(X)\n",
        "\n",
        "for i in range(0,len(X)):\n",
        "    x.append(X[i][0])\n",
        "    y.append(X[i][1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQGY6hdhw2vK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clusterer = hdbscan.HDBSCAN(min_cluster_size=15).fit(X)\n",
        "color_palette = sns.color_palette('deep', 8)\n",
        "cluster_colors = [color_palette[x] if x >= 0\n",
        "                  else (0.5, 0.5, 0.5)\n",
        "                  for x in clusterer.labels_]\n",
        "cluster_member_colors = [sns.desaturate(x, p) for x, p in\n",
        "                         zip(cluster_colors, clusterer.probabilities_)]\n",
        "\n",
        "plt.scatter(*X.T, s=50, linewidth=0, c=cluster_member_colors, alpha=0.25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnLKuyVKw2vo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#db = DBSCAN(eps=0.1, min_samples=3).fit(X)\n",
        "#clusterer = hdbscan.HDBSCAN(min_cluster_size=15).fit(X)\n",
        "hdb= hdbscan.HDBSCAN(min_samples=15,gen_min_span_tree=True).fit(X)\n",
        "hcore_samples_mask = np.zeros_like(hdb.labels_, dtype=bool)\n",
        "#core_samples_mask[db.core_sample_indices_] = True\n",
        "hlabels = hdb.labels_\n",
        "hn_clusters_ = len(set(hlabels)) - (1 if -1 in hlabels else 0)\n",
        "hunique_labels = set(hlabels)\n",
        "\n",
        "plt.figure(num=None, facecolor='w', edgecolor='k')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgXW7evIw2wF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for k in hunique_labels:\n",
        "    col=[0,0.5,1,1]\n",
        "    if k == -1:\n",
        "        col = [1, 0, 0, 1]\n",
        "    hclass_member_mask = (hlabels == k)\n",
        "\n",
        "    xy = X[hclass_member_mask & hcore_samples_mask]\n",
        "    plt.plot(xy[:, 0], xy[:, 1], 'o', color=tuple(col),markersize=5, alpha=0.5)\n",
        "\n",
        "    xy = X[hclass_member_mask & ~hcore_samples_mask]\n",
        "    plt.plot(xy[:, 0], xy[:, 1], 'o', color=tuple(col), markersize=5, alpha=0.5)\n",
        "\n",
        "plt.title('Estimated number of clusters: %d' % hn_clusters_)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "e9afddcc-9ec5-46dd-b569-ed4780af9cfd",
        "_uuid": "85c83e84b246a6715b9452349fa20d56b52bc5ae",
        "id": "cz7SM5QMw2wW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.distplot(hdb.outlier_scores_[np.isfinite(clusterer.outlier_scores_)], rug=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAKlUEsQw2wx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#hdb.condensed_tree_.plot()\n",
        "\n",
        "hdb.condensed_tree_.plot(select_clusters=True, selection_palette=sns.color_palette('deep', 16))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xvrXVO5w2xJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hdb.minimum_spanning_tree_.plot(edge_cmap='viridis',\n",
        "                                      edge_alpha=0.6,\n",
        "                                      node_size=80,\n",
        "                                      edge_linewidth=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6X9490hw2xf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#hdb = hdbscan.HDBSCAN(min_cluster_size=15).fit(X)\n",
        "color_palette = sns.color_palette('bright', 12)\n",
        "cluster_colors = [color_palette[x] if x >= 0\n",
        "                  else (0.5, 0.5, 0.5)\n",
        "                  for x in hdb.labels_]\n",
        "cluster_member_colors = [sns.desaturate(x, p) for x, p in\n",
        "                         zip(cluster_colors, hdb.probabilities_)]\n",
        "plt.scatter(*X.T, s=50, linewidth=0, c=cluster_member_colors, alpha=0.25)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2T2D0tQdw2x4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "threshold = pd.Series(hdb.outlier_scores_).quantile(0.9)\n",
        "outliers = np.where(hdb.outlier_scores_ > threshold)[0]\n",
        "plt.scatter(*X.T, s=50, linewidth=0, c='gray', alpha=0.25)\n",
        "plt.scatter(*X[outliers].T, s=50, linewidth=0, c='red', alpha=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTOZi0qww2yO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clusterer.single_linkage_tree_.plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzNY0_uVw2yo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}